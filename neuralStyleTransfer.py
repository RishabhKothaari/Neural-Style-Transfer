# -*- coding: utf-8 -*-
"""Neural-Algorithm-of-Artistic-Style-Transfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S4p4FUUXmUQTJYjFStblrNOhEHL0hCXG
"""

"""Install opencv
# https://opencv.org/
!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python
"""

import cv2

from os import path
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())

accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'

"""
Install PyTorch
!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision

!pip install Pillow==4.0.0
!pip install PIL
!pip install image

!pip install -U -q PyDrive

!pip install tqdm
"""
"""Install tqdm
!pip install tqdm
"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials


# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# PyDrive reference:
# https://googledrive.github.io/PyDrive/docs/build/html/index.html

from google.colab import auth
auth.authenticate_user()

from googleapiclient.discovery import build
drive_service = build('drive', 'v3')

"""
Download the file we just uploaded.
Replace the assignment below with your file ID
to download a different file.
A file ID looks like: 1uBtlaggVyWshwcyP6kEI-y_W3P8D26sz
"""
#content file Id
contentId = ''
#style file Id
styleId = ''
"""Download images from Google Drive using Drive python client"""
def images(file_id):

  from googleapiclient.http import MediaIoBaseDownload

  request = drive_service.files().get_media(fileId=file_id)
  downloaded = io.BytesIO()
  downloader = MediaIoBaseDownload(downloaded, request)
  done = False
  while done is False:
    # _ is a placeholder for a progress object that we ignore.
    # (Our file is small, so we skip reporting progress.)
    _, done = downloader.next_chunk()
  downloaded.seek(0)
  return downloaded

import base64
import io
import torch
import torch.nn as nn
import torchvision
import matplotlib.pyplot
import matplotlib.image as mpimg
import numpy
from PIL import Image
import tqdm

tqdm.monitor_interval = 0

height = 512
width = 512


contentData = io.BytesIO(images(contentId).read())
contentData = mpimg.imread(contentData, format='JPG')
contentImage = numpy.asarray(contentData,dtype=numpy.uint8)
contentImage = cv2.resize(contentImage,dsize=(height,width))

styleData = io.BytesIO(images(styleId).read())
styleData = mpimg.imread(styleData, format='JPG')
styleImage = numpy.asarray(styleData,dtype=numpy.uint8)

contentImg = Image.fromarray(contentImage)
styleImg = Image.fromarray(styleImage)


# specifying the transform according to the pretrained model
# also defining the inverse that denormalizes the output

preprocess_input = torchvision.transforms.Compose([
	torchvision.transforms.Resize(512),
	torchvision.transforms.ToTensor(),
	torchvision.transforms.Normalize(mean=[ 0.485, 0.456, 0.406 ], std=[ 1.0, 1.0, 1.0 ]),
	torchvision.transforms.Normalize(mean=[ 0.0, 0.0, 0.0 ], std=[ 0.229, 0.224, 0.225 ])
])

postprocess_output = torchvision.transforms.Compose([
	torchvision.transforms.Normalize(mean=[ 0.0, 0.0, 0.0 ], std=[ 1.0 / 0.229, 1.0 / 0.224, 1.0 / 0.225 ]),
	torchvision.transforms.Normalize(mean=[ -0.485, -0.456, -0.406 ], std=[ 1.0, 1.0, 1.0 ]),
	torchvision.transforms.Lambda(lambda x: x.clamp(0.0, 1.0)),
	torchvision.transforms.ToPILImage()
])

# defining the networks to extract the style and the content
# notice setting requires_grad to false to avoid updating its parameters
# afterwards creating an instance of it and making it cuda aware

class Network(torch.nn.Module):
  def __init__(self):
    super(Network, self).__init__()

    self.moduleVgg = torchvision.models.vgg19(pretrained=True)

    for parameterAll in self.moduleVgg.parameters():
      parameterAll.requires_grad = False
    # end

    class Gram(torch.nn.Module):
      def __init__(self):
        super(Gram, self).__init__()
      # end

      def forward(self, x):
        x = x.view(x.size(0), x.size(1), -1)
        x = torch.bmm(x, x.transpose(1, 2))
        return x
      # end
    # end

    self.moduleGram = Gram()
  # end

  def forward(self, x):
    variableContent = []
    variableStyle = []

    x = torch.nn.functional.relu(self.moduleVgg.features[0](x))
    variableStyle.append(self.moduleGram(x))
    x = torch.nn.functional.relu(self.moduleVgg.features[2](x))
    x = torch.nn.functional.avg_pool2d(x, kernel_size=2)
    x = torch.nn.functional.relu(self.moduleVgg.features[5](x))
    variableStyle.append(self.moduleGram(x))
    variableContent.append(x)
    x = torch.nn.functional.relu(self.moduleVgg.features[7](x))
    x = torch.nn.functional.avg_pool2d(x, kernel_size=2)
    x = torch.nn.functional.relu(self.moduleVgg.features[10](x))
    variableStyle.append(self.moduleGram(x))
    x = torch.nn.functional.relu(self.moduleVgg.features[12](x))
    x = torch.nn.functional.relu(self.moduleVgg.features[14](x))
    x = torch.nn.functional.relu(self.moduleVgg.features[16](x))
#     variableContent.append(x)
    x = torch.nn.functional.avg_pool2d(x, kernel_size=2)
    x = torch.nn.functional.relu(self.moduleVgg.features[19](x))
    variableStyle.append(self.moduleGram(x))
    x = torch.nn.functional.relu(self.moduleVgg.features[21](x))
    x = torch.nn.functional.relu(self.moduleVgg.features[23](x))
    x = torch.nn.functional.relu(self.moduleVgg.features[25](x))
    x = torch.nn.functional.avg_pool2d(x, kernel_size=2)
    x = torch.nn.functional.relu(self.moduleVgg.features[28](x)) 
    variableStyle.append(self.moduleGram(x))
    return variableContent, variableStyle
  # end
# end

network = Network().cuda()

# loading the style as well as the content image, try others for fun

contentImage = torch.autograd.Variable(data=preprocess_input(contentImg).unsqueeze(0).cuda(),requires_grad=False)
styleImage = torch.autograd.Variable(data=preprocess_input(styleImg).unsqueeze(0).cuda(),requires_grad=False)

# obtaining the style and content representations of the input images

contentRepresentation = [A.detach() for A in network(contentImage)[0]]
styleRepresentation = [A.detach() for A in network(styleImage)[1]]

# creating the output that will be iteratively updated accordingly
# try torch.randn, this requires increasing dblAlpha to 1.0 though

output = torch.autograd.Variable(data=contentImage.data.clone(),requires_grad=True)

# output = torch.autograd.Variable(data=torch.randn((3,512,512)).unsqueeze(0).cuda(),requires_grad=True)

# specifying the optimizer, lbfgs seems to work best with style transfer
# notice that it receives the output image as the tunable parameter

optimizer = torch.optim.LBFGS(params=[output],lr=0.2)

# specifies the weight for the style  loss as well as the content loss
# the style loss is weighted to normalize contribution from each layer

alpha = 1.0
beta = 1.0

contentWeight = [1.0]

styleWeight = [ 1.0 / intSize for intSize in [ 64 * 512 * 512, 128 * 256 * 256, 256 * 128 * 128, 512 * 64 * 64, 512 * 32 * 32 ] ]

# styleWeight = [1.0/(n*n) for n in [64,128,256,512,512]]

# iteratively updating output image according to the loss functions
generatedImages = []

for i in tqdm.tqdm(range(100)):
  def optimize():
    optimizer.zero_grad()
    
    contentEstimates,styleEstimates = network(output)
    
    contentLoss = sum([contentWeight[i]*torch.nn.functional.mse_loss(input=estimates,target=contentRepresentation[i]) for i,estimates in enumerate(contentEstimates)])
    styleLoss = sum([styleWeight[i]*torch.nn.functional.mse_loss(input=estimates,target=styleRepresentation[i]) for i,estimates in enumerate(styleEstimates)])
    loss = contentLoss + styleLoss
    
    if i % 10 == 0:
      print('')
      print('content: ' + str(contentLoss.data[0]))
      print('style: ' + str(styleLoss.data[0]))
      print('')
    #end
    loss.backward()
    return loss
  #end
  
  if i % 10 == 0:
    matplotlib.pyplot.imshow(postprocess_output(output.data[0].cpu().squeeze()))
    generatedImages.append(postprocess_output(output.data[0].cpu().squeeze()))
  #end
  
  optimizer.step(optimize)
#end

matplotlib.pyplot.grid(False)
matplotlib.pyplot.imshow(postprocess_output(output.data[0].cpu().squeeze()))
matplotlib.pyplot.show()